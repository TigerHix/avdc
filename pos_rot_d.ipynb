{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import wandb\n",
    "\n",
    "print(sys.executable)\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate Dataset\n",
    "(No need to run this, dataset is under `./dataset`)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f397081de0f9c68"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e46ab3b7b0381d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "mmd_path = '/mnt/c/research/avdc/mmd_processed2'\n",
    "sliding_window_size = 10\n",
    "val_ratio = 0.05\n",
    "\n",
    "train_data = []\n",
    "val_data = []\n",
    "\n",
    "def process(folder_path):\n",
    "    # Iterate through all json files in the folder\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith('.json'):\n",
    "            with open(folder_path + '/' + file, 'r') as f:\n",
    "                shots = json.load(f)\n",
    "    \n",
    "                duration = 0\n",
    "                for shot in shots:\n",
    "                    duration += shot['duration']\n",
    "            \n",
    "                array = np.zeros((len(shots), 1 + 3 * 4 + 4 * 4))\n",
    "                for i, shot in enumerate(shots):\n",
    "                    array[i, 0] = shot['duration']\n",
    "                    array[i, 1:4] = shot['character_pos']['x'], shot['character_pos']['y'], shot['character_pos']['z']\n",
    "                    array[i, 4:7] = shot['character_pos_offset']['x'], shot['character_pos_offset']['y'], shot['character_pos_offset']['z']\n",
    "                    array[i, 7] = np.deg2rad(shot['character_rot_y'])\n",
    "                    array[i, 8] = shot['character_rot_y_offset']\n",
    "                    \n",
    "                    array[i, 9:12] = shot['camera_pos']['x'], shot['camera_pos']['y'], shot['camera_pos']['z']\n",
    "                    array[i, 12:15] = shot['camera_pos_offset']['x'], shot['camera_pos_offset']['y'], shot['camera_pos_offset']['z']\n",
    "                    array[i, 15:18] = np.deg2rad(shot['camera_rot']['x']), np.deg2rad(shot['camera_rot']['y']), np.deg2rad(shot['camera_rot']['z'])\n",
    "                    array[i, 18:21] = np.deg2rad(shot['camera_rot_offset']['x']), np.deg2rad(shot['camera_rot_offset']['y']), np.deg2rad(shot['camera_rot_offset']['z'])\n",
    "                    array[i, 21] = shot['camera_distance']\n",
    "                    array[i, 22] = shot['camera_distance_offset']\n",
    "                    array[i, 23] = shot['camera_fov']\n",
    "                    array[i, 24] = shot['camera_fov_offset']\n",
    "                    \n",
    "                for i in range(len(array) - sliding_window_size + 1):\n",
    "                    data = train_data if np.random.rand() > val_ratio else val_data\n",
    "                    data.append(array[i:i+sliding_window_size])\n",
    "    \n",
    "    return True\n",
    "\n",
    "processed = 0\n",
    "for folder in os.listdir(mmd_path):\n",
    "    processed += 1 if process(mmd_path + '/' + folder) else 0\n",
    "\n",
    "print(\"Generated \", len(train_data), \"training samples and \", len(val_data), \"validation samples from \", processed, \"folders.\")\n",
    "\n",
    "np_data = np.array(train_data)\n",
    "np.save('./dataset/train.npy', np_data)\n",
    "np_data = np.array(val_data)\n",
    "np.save('./dataset/val.npy', np_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset Definition"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73445189af15dff5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affc13680a5b5c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class AVDCDataset(Dataset):\n",
    "    def __init__(self, dataset_path):\n",
    "        self.shots = []\n",
    "        self.load_dataset(dataset_path)\n",
    "\n",
    "    def load_dataset(self, dataset_path):\n",
    "        self.shots = np.load(dataset_path)\n",
    "        self.shots = torch.from_numpy(self.shots).float()\n",
    "        \n",
    "        #for i in range(0, len(self.shots[0])):\n",
    "            # Base camera coordinate on character position TODO: Really necessary?\n",
    "            # self.shots[:, i, 7:10] -= self.shots[:, i, 1:4]\n",
    "            \n",
    "            # Find max and min from 1:4 TODO: Really necessary?\n",
    "            # max_pos = self.shots[:, i, 1:4].max()\n",
    "            # min_pos = self.shots[:, i, 1:4].min()\n",
    "            # scale = max_pos - min_pos\n",
    "            # self.shots[:, i, 1:4] = (self.shots[:, i, 1:4] - min_pos) / scale\n",
    "            # self.shots[:, i, 7:10] = (self.shots[:, i, 7:10] - min_pos) / scale\n",
    "            # self.shots[:, i, 4:7] = self.shots[:, i, 4:7] / scale\n",
    "            # self.shots[:, i, 10:13] = self.shots[:, i, 10:13] / scale\n",
    "            \n",
    "            # Rotation needs no changes\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.shots.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        shots = self.shots[idx]\n",
    "        inputs = shots[:, 0:9]\n",
    "        labels = shots[:, 9:25]\n",
    "        return inputs, labels\n",
    "\n",
    "# Model, loss, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Data loading\n",
    "train_dataset = AVDCDataset(\"./dataset/train.npy\")\n",
    "val_dataset = AVDCDataset(\"./dataset/val.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948161eba15fa87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 3]) torch.Size([3, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([0.1675, 2.0369])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def delta_pos(rot, d):\n",
    "    # Calculate cosines and sines for each rotation angle in the batch\n",
    "    cos_rot = torch.cos(rot)\n",
    "    sin_rot = torch.sin(rot)\n",
    "    \n",
    "    # Prepare zeros and ones for the batch\n",
    "    zeros = torch.zeros_like(d)\n",
    "    ones = torch.ones_like(d)\n",
    "\n",
    "    # Define the rotation matrices for each axis with batched operations\n",
    "    # Rz matrices for the whole batch\n",
    "    Rz_batch = torch.stack([\n",
    "        torch.stack([cos_rot[..., 2], -sin_rot[..., 2], zeros], dim=-1),\n",
    "        torch.stack([sin_rot[..., 2], cos_rot[..., 2], zeros], dim=-1),\n",
    "        torch.stack([zeros, zeros, ones], dim=-1)\n",
    "    ], dim=-2)  # No need for transpose as we build it correctly\n",
    "    \n",
    "    # Ry matrices for the whole batch\n",
    "    Ry_batch = torch.stack([\n",
    "        torch.stack([cos_rot[..., 1], zeros, sin_rot[..., 1]], dim=-1),\n",
    "        torch.stack([zeros, ones, zeros], dim=-1),\n",
    "        torch.stack([-sin_rot[..., 1], zeros, cos_rot[..., 1]], dim=-1)\n",
    "    ], dim=-2)\n",
    "    \n",
    "    # Rx matrices for the whole batch\n",
    "    Rx_batch = torch.stack([\n",
    "        torch.stack([ones, zeros, zeros], dim=-1),\n",
    "        torch.stack([zeros, cos_rot[..., 0], -sin_rot[..., 0]], dim=-1),\n",
    "        torch.stack([zeros, sin_rot[..., 0], cos_rot[..., 0]], dim=-1)\n",
    "    ], dim=-2)\n",
    "    \n",
    "    # Compute the combined rotation matrix for each set of rotations in the batch\n",
    "    # The order of multiplication might need to be adjusted based on your specific rotation order requirements\n",
    "    R_ZXY_batch = torch.einsum('...ij,...jk->...ik', Rz_batch, Ry_batch)\n",
    "    R_ZXY_batch = torch.einsum('...ij,...jk->...ik', R_ZXY_batch, Rx_batch)\n",
    "    \n",
    "    # Apply the rotation to the distance vector for each item in the batch\n",
    "    d_vec = torch.stack([zeros, zeros, d], axis=-1)  # Create a batch of distance vectors\n",
    "    camera_positions = torch.einsum('...ij,...j->...i', R_ZXY_batch, d_vec)\n",
    "    \n",
    "    return camera_positions\n",
    "\n",
    "batched_camera_rot = torch.Tensor(np.deg2rad(np.array([\n",
    "    [[-22.69, 0.00, 0.00], [-22.69, 0.00, 0.00]],\n",
    "    [[0.00, 0.00, -5.41], [-22.69, 0.00, 0.00]],\n",
    "    [[-22.69, 0.00, 0.00], [0.00, 0.00, 0.00]]\n",
    "])))\n",
    "batched_camera_distance = torch.Tensor(np.array([\n",
    "    [4.559982, 4.559982],\n",
    "    [2.159997, 4.559982],\n",
    "    [4.559982, 1.199999]\n",
    "]))\n",
    "\n",
    "print(batched_camera_rot.shape, batched_camera_distance.shape)\n",
    "\n",
    "batched_result = delta_pos(batched_camera_rot, batched_camera_distance)\n",
    "batched_result\n",
    "\n",
    "# Expected\n",
    "# array([[[0.        , 1.75899036, 4.20706415],\n",
    "#         [0.        , 1.75899036, 4.20706415]],\n",
    "# \n",
    "#        [[0.        , 0.        , 2.159997  ],\n",
    "#         [0.        , 1.75899036, 4.20706415]],\n",
    "# \n",
    "#        [[0.        , 1.75899036, 4.20706415],\n",
    "#         [0.        , 0.        , 1.199999  ]]])\n",
    "\n",
    "character_chest_height = 1.115\n",
    "\n",
    "def calculate_angle(camera_pos, camera_rot, character_pos):\n",
    "    chest_pos = character_pos.clone()\n",
    "    chest_pos[..., 1] = 1.115\n",
    "\n",
    "    dir_vec = chest_pos - camera_pos\n",
    "    dir_norm = torch.norm(dir_vec, dim=-1, keepdim=True)\n",
    "    dir_vec = dir_vec / (dir_norm + 1e-6)  # Add epsilon to avoid division by zero\n",
    "\n",
    "    cos_rot = torch.cos(camera_rot)\n",
    "    sin_rot = torch.sin(camera_rot)\n",
    "\n",
    "    Rz = torch.stack([\n",
    "        torch.stack([cos_rot[..., 2], -sin_rot[..., 2], torch.zeros_like(cos_rot[..., 2])], dim=-1),\n",
    "        torch.stack([sin_rot[..., 2], cos_rot[..., 2], torch.zeros_like(sin_rot[..., 2])], dim=-1),\n",
    "        torch.stack([torch.zeros_like(cos_rot[..., 2]), torch.zeros_like(sin_rot[..., 2]), torch.ones_like(sin_rot[..., 2])], dim=-1)\n",
    "    ], dim=-2)\n",
    "\n",
    "    Ry = torch.stack([\n",
    "        torch.stack([cos_rot[..., 1], torch.zeros_like(cos_rot[..., 1]), sin_rot[..., 1]], dim=-1),\n",
    "        torch.stack([torch.zeros_like(cos_rot[..., 1]), torch.ones_like(cos_rot[..., 1]), torch.zeros_like(sin_rot[..., 1])], dim=-1),\n",
    "        torch.stack([-sin_rot[..., 1], torch.zeros_like(sin_rot[..., 1]), cos_rot[..., 1]], dim=-1)\n",
    "    ], dim=-2)\n",
    "\n",
    "    Rx = torch.stack([\n",
    "        torch.stack([torch.ones_like(cos_rot[..., 0]), torch.zeros_like(cos_rot[..., 0]), torch.zeros_like(sin_rot[..., 0])], dim=-1),\n",
    "        torch.stack([torch.zeros_like(cos_rot[..., 0]), cos_rot[..., 0], -sin_rot[..., 0]], dim=-1),\n",
    "        torch.stack([torch.zeros_like(cos_rot[..., 0]), sin_rot[..., 0], cos_rot[..., 0]], dim=-1)\n",
    "    ], dim=-2)\n",
    "\n",
    "    R = torch.einsum('...ij,...jk->...ik', Rz, Ry)\n",
    "    R = torch.einsum('...ij,...jk->...ik', R, Rx)\n",
    "\n",
    "    forward_vec = torch.einsum('...ij,...j->...i', R, torch.tensor([0.0, 0.0, 1.0], device=dir_vec.device, dtype=torch.float32))\n",
    "    forward_norm = torch.norm(forward_vec, dim=-1, keepdim=True)\n",
    "    forward_vec = forward_vec / (forward_norm + 1e-6)  # Add epsilon to avoid division by zero\n",
    "\n",
    "    cos_angle = torch.sum(forward_vec * dir_vec, dim=-1)\n",
    "    cos_angle = torch.clamp(cos_angle, -0.9999, 0.9999)  # Clamp to avoid invalid values for acos\n",
    "    angle = torch.acos(cos_angle) # * (180 / torch.pi)  # Convert radians to degrees\n",
    "\n",
    "    return angle\n",
    "    \n",
    "\n",
    "# Define the test inputs\n",
    "camera_pos_test = torch.tensor([[-3.11, 1.15, 2.01], [-1.77, 1.34, 3.77]], dtype=torch.float32)\n",
    "camera_rot_test = torch.tensor(np.deg2rad([[2.30, 112.65, 0.00], [354.39, 37.95, 0.00]]), dtype=torch.float32)  # No rotation and 90 degrees yaw\n",
    "character_pos_test = torch.tensor([[0.00, 1.11, 0.06], [0.00, 1.11, 0.06]], dtype=torch.float32)\n",
    "\n",
    "# Run the test\n",
    "calculate_angle(camera_pos_test, camera_rot_test, character_pos_test)\n",
    "\n",
    "# Expected: 0.1675, 2.0369"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T11:28:44.479287800Z",
     "start_time": "2024-04-30T11:28:43.088854900Z"
    }
   },
   "id": "45a16793cd513921"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [2] at entry 0 and [2, 3] at entry 2",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdelta_pos\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcamera_rot_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcamera_pos_test\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[8], line 13\u001B[0m, in \u001B[0;36mdelta_pos\u001B[0;34m(rot, d)\u001B[0m\n\u001B[1;32m      8\u001B[0m ones \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mones_like(d)\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Define the rotation matrices for each axis with batched operations\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# Rz matrices for the whole batch\u001B[39;00m\n\u001B[1;32m     12\u001B[0m Rz_batch \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstack([\n\u001B[0;32m---> 13\u001B[0m     \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mcos_rot\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43msin_rot\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mzeros\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m,\n\u001B[1;32m     14\u001B[0m     torch\u001B[38;5;241m.\u001B[39mstack([sin_rot[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, \u001B[38;5;241m2\u001B[39m], cos_rot[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, \u001B[38;5;241m2\u001B[39m], zeros], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m),\n\u001B[1;32m     15\u001B[0m     torch\u001B[38;5;241m.\u001B[39mstack([zeros, zeros, ones], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     16\u001B[0m ], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m)  \u001B[38;5;66;03m# No need for transpose as we build it correctly\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# Ry matrices for the whole batch\u001B[39;00m\n\u001B[1;32m     19\u001B[0m Ry_batch \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstack([\n\u001B[1;32m     20\u001B[0m     torch\u001B[38;5;241m.\u001B[39mstack([cos_rot[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, \u001B[38;5;241m1\u001B[39m], zeros, sin_rot[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, \u001B[38;5;241m1\u001B[39m]], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m),\n\u001B[1;32m     21\u001B[0m     torch\u001B[38;5;241m.\u001B[39mstack([zeros, ones, zeros], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m),\n\u001B[1;32m     22\u001B[0m     torch\u001B[38;5;241m.\u001B[39mstack([\u001B[38;5;241m-\u001B[39msin_rot[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, \u001B[38;5;241m1\u001B[39m], zeros, cos_rot[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, \u001B[38;5;241m1\u001B[39m]], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     23\u001B[0m ], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: stack expects each tensor to be equal size, but got [2] at entry 0 and [2, 3] at entry 2"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T11:28:54.937451Z",
     "start_time": "2024-04-30T11:28:54.887137500Z"
    }
   },
   "id": "ca683494eea4ae9f"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7219, 0.2308, 1.8508]])\n",
      "tensor([[ 3.8489, -0.9673,  4.5000]])\n",
      "tensor([[ 2.0035, -5.1866,  0.0883]])\n",
      "tensor([[-2.0481, -2.6924,  0.0271]])\n"
     ]
    }
   ],
   "source": [
    " # \"camera_pos\": {\n",
    " #    \"x\": -0.193416685,\n",
    " #    \"y\": 1.41547632,\n",
    " #    \"z\": 0.180194452\n",
    " #  },\n",
    " #  \"camera_pos_offset\": {\n",
    " #    \"x\": -0.303891361,\n",
    " #    \"y\": -0.9445602,\n",
    " #    \"z\": -0.524982154\n",
    " #  },\n",
    " #  \"camera_rot\": {\n",
    " #    \"x\": 87.1031952,\n",
    " #    \"y\": 288.31192,\n",
    " #    \"z\": 383.871033\n",
    " #  },\n",
    " #  \"camera_rot_offset\": {\n",
    " #    \"x\": -359.080658,\n",
    " #    \"y\": -4.89764547,\n",
    " #    \"z\": 300.791779\n",
    " #  },\n",
    " #  \"camera_distance\": 5.56078148,\n",
    " #  \"camera_distance_offset\": -2.1778245,\n",
    " \n",
    "camera_rot = torch.tensor(np.deg2rad([[10, 20, 45]]), dtype=torch.float32)\n",
    "camera_rot_offset = torch.tensor(np.deg2rad([[20, 10, -10]]), dtype=torch.float32)\n",
    "camera_distance = torch.tensor([2], dtype=torch.float32)\n",
    "camera_distance_offset = torch.tensor([4], dtype=torch.float32)\n",
    "\n",
    "print(delta_pos(camera_rot, camera_distance))\n",
    "print(delta_pos(camera_rot + camera_rot_offset, camera_distance + camera_distance_offset))\n",
    "\n",
    "camera_pos = torch.tensor([[-0.193416685, 1.41547632, 0.180194452]], dtype=torch.float32)\n",
    "camera_pos_offset = torch.tensor([[-0.303891361, -0.9445602, -0.524982154]], dtype=torch.float32)\n",
    "camera_rot = torch.tensor(np.deg2rad([[87.1031952, 288.31192, 383.871033]]), dtype=torch.float32)\n",
    "camera_rot_offset = torch.tensor(np.deg2rad([[-359.080658, -4.89764547, 300.791779]]), dtype=torch.float32)\n",
    "camera_distance = torch.tensor([5.56078148], dtype=torch.float32)\n",
    "camera_distance_offset = torch.tensor([-2.1778245], dtype=torch.float32)\n",
    "\n",
    "print(delta_pos(camera_rot, camera_distance))\n",
    "print(delta_pos(camera_rot + camera_rot_offset, camera_distance + camera_distance_offset))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T11:30:28.412161800Z",
     "start_time": "2024-04-30T11:30:28.369486400Z"
    }
   },
   "id": "49f39e368df4f837"
  },
  {
   "cell_type": "markdown",
   "source": [
    "19## Actual Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "380ac79de67cd4c1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d882b58f32a460e4",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.pe[:, : x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class AVDCEncoderOnlyModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model, dim_feedforward, num_heads, num_encoder_layers, dropout=0.1):\n",
    "        super(AVDCEncoderOnlyModel, self).__init__()\n",
    "\n",
    "        self.input_embedding = nn.Linear(input_dim, d_model)\n",
    "        self.input_positional_encoding = PositionalEncoding(d_model)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_encoder_layers)\n",
    "        self.out = nn.Linear(d_model, output_dim)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.input_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.out.weight)\n",
    "        self.input_embedding.bias.data.fill_(0.01)\n",
    "        self.out.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward(self, x, labels, inference):\n",
    "        x = self.input_embedding(x)\n",
    "        x = self.input_positional_encoding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "def compute_loss(inputs, outputs, labels):\n",
    "    var_loss_weight = 0.05\n",
    "    angle_weight = 10\n",
    "    y_weight = 5\n",
    "    \n",
    "    # ensures predicted position delta is similar to true position delta (L2 loss)\n",
    "    pos_delta_loss = (torch.norm(outputs[:, :, 3:6], dim=2) - torch.norm(labels[:, :, 3:6], dim=2)) ** 2\n",
    "    \n",
    "    # ensures predicted start camera position has similar distance to character\n",
    "    predicted_start = outputs[:, :, 0:3] + delta_pos(outputs[:, :, 6:9], outputs[:, :, 12])\n",
    "    true_start = labels[:, :, 0:3] + delta_pos(labels[:, :, 6:9], labels[:, :, 12])\n",
    "    predicted_start_dist_to_char = torch.norm(predicted_start - inputs[:, :, 1:4], dim=2)\n",
    "    true_start_dist_to_char = torch.norm(true_start - inputs[:, :, 1:4], dim=2)\n",
    "    start_distance_loss = (predicted_start_dist_to_char - true_start_dist_to_char) ** 2\n",
    "    \n",
    "    # ensures predicted middle camera position has similar distance to character\n",
    "    predicted_middle = (outputs[:, :, 0:3] + outputs[:, :, 3:6] * 0.5) + delta_pos(outputs[:, :, 6:9] + outputs[:, :, 9:12] * 0.5, outputs[:, :, 12] + outputs[:, :, 13] * 0.5)\n",
    "    true_middle = (labels[:, :, 0:3] + labels[:, :, 3:6] * 0.5) + delta_pos(labels[:, :, 6:9] + labels[:, :, 9:12] * 0.5, labels[:, :, 12] + labels[:, :, 13] * 0.5)\n",
    "    predicted_middle_dist_to_char = torch.norm(predicted_middle - (inputs[:, :, 1:4] + inputs[:, :, 4:7] * 0.5), dim=2)\n",
    "    true_middle_dist_to_char = torch.norm(true_middle - (inputs[:, :, 1:4] + inputs[:, :, 4:7] * 0.5), dim=2)\n",
    "    middle_distance_loss = (predicted_middle_dist_to_char - true_middle_dist_to_char) ** 2\n",
    "    \n",
    "    # ensures predicted end camera position has similar distance to character\n",
    "    predicted_end = (outputs[:, :, 0:3] + outputs[:, :, 3:6]) + delta_pos(outputs[:, :, 6:9] + outputs[:, :, 9:12], outputs[:, :, 12] + outputs[:, :, 13])\n",
    "    true_end = (labels[:, :, 0:3] + labels[:, :, 3:6]) + delta_pos(labels[:, :, 6:9] + labels[:, :, 9:12], labels[:, :, 12] + labels[:, :, 13])\n",
    "    predicted_end_dist_to_char = torch.norm(predicted_end - (inputs[:, :, 1:4] + inputs[:, :, 4:7]), dim=2)\n",
    "    true_end_dist_to_char = torch.norm(true_end - (inputs[:, :, 1:4] + inputs[:, :, 4:7]), dim=2)\n",
    "    end_distance_loss = (predicted_end_dist_to_char - true_end_dist_to_char) ** 2\n",
    "    \n",
    "    # ensures predicted start camera rotation has similar angle\n",
    "    predicted_start_angle = calculate_angle(predicted_start, outputs[:, :, 6:9], inputs[:, :, 1:4])\n",
    "    true_start_angle = calculate_angle(true_start, labels[:, :, 6:9], inputs[:, :, 1:4])\n",
    "    start_angle_loss = (predicted_start_angle - true_start_angle) ** 2\n",
    "    \n",
    "    # ensures predicted middle camera rotation has similar angle\n",
    "    predicted_middle_angle = calculate_angle(predicted_middle, outputs[:, :, 6:9] + outputs[:, :, 9:12] * 0.5, inputs[:, :, 1:4] + inputs[:, :, 4:7] * 0.5)\n",
    "    true_middle_angle = calculate_angle(true_middle, labels[:, :, 6:9] + labels[:, :, 9:12] * 0.5, inputs[:, :, 1:4] + inputs[:, :, 4:7] * 0.5)\n",
    "    middle_angle_loss = (predicted_middle_angle - true_middle_angle) ** 2\n",
    "    \n",
    "    # ensures predicted end camera rotation has similar angle\n",
    "    predicted_end_angle = calculate_angle(predicted_end, outputs[:, :, 6:9] + outputs[:, :, 9:12], inputs[:, :, 1:4] + inputs[:, :, 4:7])\n",
    "    true_end_angle = calculate_angle(true_end, labels[:, :, 6:9] + labels[:, :, 9:12], inputs[:, :, 1:4] + inputs[:, :, 4:7])\n",
    "    end_angle_loss = (predicted_end_angle - true_end_angle) ** 2\n",
    "    \n",
    "    # ensures camera start y is similar\n",
    "    predicted_start_y = outputs[:, :, 1]\n",
    "    true_start_y = labels[:, :, 1]\n",
    "    start_y_loss = (predicted_start_y - true_start_y) ** 2\n",
    "    \n",
    "    # ensures camera end y is similar\n",
    "    predicted_end_y = outputs[:, :, 1] + outputs[:, :, 4]\n",
    "    true_end_y = labels[:, :, 1] + labels[:, :, 4]\n",
    "    end_y_loss = (predicted_end_y - true_end_y) ** 2\n",
    "    \n",
    "    # # ensures camera start FOV is similar\n",
    "    # predicted_start_fov = outputs[:, :, 14]\n",
    "    # true_start_fov = labels[:, :, 14]\n",
    "    # start_fov_loss = (predicted_start_fov - true_start_fov) ** 2\n",
    "    # \n",
    "    # # ensures camera end FOV is similar\n",
    "    # predicted_end_fov = outputs[:, :, 14] + outputs[:, :, 15]\n",
    "    # true_end_fov = labels[:, :, 14] + labels[:, :, 15]\n",
    "    # end_fov_loss = (predicted_end_fov - true_end_fov) ** 2\n",
    "    \n",
    "    # ensures predicted start position has enough variance\n",
    "    predicted_variance = torch.var(predicted_start, dim=1, unbiased=False)\n",
    "    predicted_variance = torch.mean(predicted_variance, dim=-1)\n",
    "    true_variance = torch.var(true_start, dim=1, unbiased=False)\n",
    "    true_variance = torch.mean(true_variance, dim=-1)\n",
    "    variance_loss = (predicted_variance - true_variance) ** 2\n",
    "    \n",
    "    # ensures start delta has similar variance to true start delta\n",
    "    predicted_start_delta = outputs[:, :, 3:6]\n",
    "    true_start_delta = labels[:, :, 3:6]\n",
    "    predicted_start_delta_variance = torch.var(predicted_start_delta, dim=1, unbiased=False)\n",
    "    predicted_start_delta_variance = torch.mean(predicted_start_delta_variance, dim=-1)\n",
    "    true_start_delta_variance = torch.var(true_start_delta, dim=1, unbiased=False)\n",
    "    true_start_delta_variance = torch.mean(true_start_delta_variance, dim=-1)\n",
    "    variance_loss += (predicted_start_delta_variance - true_start_delta_variance) ** 2\n",
    "    \n",
    "    # print(\"Pos delta loss\", pos_delta_loss.mean())\n",
    "    # print(\"Start distance loss\", start_distance_loss.mean())\n",
    "    # print(\"Middle distance loss\", middle_distance_loss.mean())\n",
    "    # print(\"End distance loss\", end_distance_loss.mean())\n",
    "    # print(\"Angle loss\", angle_weight * (start_angle_loss.mean() + middle_angle_loss.mean() + end_angle_loss.mean()))\n",
    "    # print(\"Y loss\", y_weight * (start_y_loss.mean() + end_y_loss.mean()))\n",
    "    # print(\"Variance loss\", var_loss_weight * variance_loss.mean())\n",
    "    return (pos_delta_loss.mean() + \n",
    "            start_distance_loss.mean() +\n",
    "            middle_distance_loss.mean() +\n",
    "            end_distance_loss.mean() + \n",
    "            angle_weight * (start_angle_loss.mean() + middle_angle_loss.mean() + end_angle_loss.mean()) +\n",
    "            y_weight * (start_y_loss.mean() + end_y_loss.mean()) +\n",
    "            var_loss_weight * variance_loss.mean())\n",
    "\n",
    "def compute_mse_loss(inputs, outputs, labels):\n",
    "    return nn.MSELoss()(outputs[0:12], labels[0:12])\n",
    "\n",
    "def visualize(inputs, outputs, labels):\n",
    "    rand_idx = np.random.randint(0, outputs.shape[0])\n",
    "    inputs = inputs[rand_idx]\n",
    "    outputs = outputs[rand_idx]\n",
    "    labels = labels[rand_idx]\n",
    "    \n",
    "    # Draw camera path on XZ plane\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.ioff()\n",
    "    \n",
    "    character_pos_x_from = inputs[:, 1].cpu().detach().numpy()\n",
    "    character_pos_z_from = inputs[:, 3].cpu().detach().numpy()\n",
    "    character_pos_x_to = character_pos_x_from + inputs[:, 4].cpu().detach().numpy()\n",
    "    character_pos_z_to = character_pos_z_from + inputs[:, 6].cpu().detach().numpy()\n",
    "    \n",
    "    output_camera_pos = outputs[:, 0:3]\n",
    "    output_camera_pos_offset = outputs[:, 3:6]\n",
    "    output_camera_rot = outputs[:, 6:9]\n",
    "    output_camera_rot_offset = outputs[:, 9:12]\n",
    "    output_camera_d = outputs[:, 12]\n",
    "    output_camera_d_offset = outputs[:, 13]\n",
    "    \n",
    "    # output_camera_pos_x_from = output_camera_pos_from[:, 0].cpu().detach().numpy()\n",
    "    # output_camera_pos_z_from = output_camera_pos_from[:, 2].cpu().detach().numpy()\n",
    "    # output_camera_pos_x_to = output_camera_pos_to[:, 0].cpu().detach().numpy()\n",
    "    # output_camera_pos_z_to = output_camera_pos_to[:, 2].cpu().detach().numpy()\n",
    "    \n",
    "    labels_camera_pos = labels[:, 0:3]\n",
    "    labels_camera_pos_offset = labels[:, 3:6]\n",
    "    labels_camera_rot = labels[:, 6:9]\n",
    "    labels_camera_rot_offset = labels[:, 9:12]\n",
    "    labels_camera_d = labels[:, 12]\n",
    "    labels_camera_d_offset = labels[:, 13]\n",
    "    \n",
    "    num_paths = outputs.shape[0]\n",
    "    for i in range(num_paths):\n",
    "        ax.plot([character_pos_x_from[i], character_pos_x_to[i]], [character_pos_z_from[i], character_pos_z_to[i]], label=\"Character path\", color=np.array([0, 1, 0, i / num_paths]))\n",
    "        \n",
    "        # sample camera curve path\n",
    "        def sample(pos, pos_offset, rot, rot_offset, d, d_offset, color):\n",
    "            # for j in range(0, 100):\n",
    "            #     t = j / 100\n",
    "            #     p = pos + t * pos_offset\n",
    "            #     r = rot + t * rot_offset\n",
    "            #     d = d + t * d_offset\n",
    "            #     adjusted_pos = (p + delta_pos(r, d)).cpu().detach().numpy()\n",
    "            #     ax.scatter(adjusted_pos[0], adjusted_pos[2], color=color)\n",
    "        \n",
    "            # batched version\n",
    "            t = torch.linspace(0, 1, steps=100).unsqueeze(1).to(device)\n",
    "            p = pos + t * pos_offset\n",
    "            r = rot + t * rot_offset\n",
    "            d = (d + t * d_offset).squeeze(-1)\n",
    "            adjusted_pos = (p + delta_pos(r, d)).cpu().detach().numpy()\n",
    "            ax.plot(adjusted_pos[:, 0], adjusted_pos[:, 2], color=color)\n",
    "        \n",
    "        # ax.plot([output_camera_pos_x_from[i], output_camera_pos_x_to[i]], [output_camera_pos_z_from[i], output_camera_pos_z_to[i]], label=\"Predicted camera path\", color=np.array([1, 0, 0, i / num_paths]))\n",
    "        # ax.plot([labels_camera_pos_x_from[i], labels_camera_pos_x_to[i]], [labels_camera_pos_z_from[i], labels_camera_pos_z_to[i]], label=\"True camera path\", color=np.array([0, 0, 1, i / num_paths]))\n",
    "        sample(output_camera_pos[i], output_camera_pos_offset[i], output_camera_rot[i], output_camera_rot_offset[i], output_camera_d[i], output_camera_d_offset[i], np.array([1, 0, 0, i / num_paths]))\n",
    "        sample(labels_camera_pos[i], labels_camera_pos_offset[i], labels_camera_rot[i], labels_camera_rot_offset[i], labels_camera_d[i], labels_camera_d_offset[i], np.array([0, 0, 1, i / num_paths]))\n",
    "    \n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Z')\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    if write_test_data:\n",
    "        import time\n",
    "        # timestamp as filename\n",
    "        filename = f\"test_data/test_{(time.time() * 1000):.0f}\"\n",
    "        fig.savefig(filename + \".png\")\n",
    "        \n",
    "        json_obj = {\n",
    "            \"inputs\": inputs.cpu().detach().numpy().tolist(),\n",
    "            \"outputs\": outputs.cpu().detach().numpy().tolist(),\n",
    "            \"labels\": labels.cpu().detach().numpy().tolist()\n",
    "        }\n",
    "        with open(f\"{filename}.json\", \"w\") as f:\n",
    "            json.dump(json_obj, f)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def evaluate(epoch, model, dataloader, optimizer):\n",
    "    if optimizer is not None:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    avg_loss = 0\n",
    "\n",
    "    with torch.set_grad_enabled(optimizer is not None):\n",
    "        i = 0\n",
    "        length = len(dataloader)\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs, labels, optimizer is None)\n",
    "            \n",
    "            if optimizer is None and (epoch > 0 or not will_train) and epoch % visualize_epochs == 0:\n",
    "                fig = visualize(inputs, outputs, labels)\n",
    "                if use_wandb:\n",
    "                    wandb.log({\"epoch\": epoch, \"path\": wandb.Image(fig)})\n",
    "                plt.close(fig)\n",
    "\n",
    "            loss = compute_loss(inputs, outputs, labels)\n",
    "            l = loss.item()\n",
    "            avg_loss += l\n",
    "            \n",
    "            if optimizer is not None:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            i += 1\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Step {i}/{length}, Loss: {l}\")\n",
    "            if use_wandb and optimizer is not None:\n",
    "                wandb.log({\"step\": epoch * length + i, \"loss\": l})\n",
    "\n",
    "    avg_loss /= len(dataloader)\n",
    "\n",
    "    return avg_loss\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.set_printoptions(sci_mode=False, threshold=10000)\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Hyperparameters\n",
    "d_model = 512\n",
    "dim_feedforward = 1024\n",
    "num_heads = 2\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 1\n",
    "dropout = 0.2\n",
    "batch_size = 1024\n",
    "epochs = 100000\n",
    "learning_rate = 5e-5\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=32, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1024, shuffle=False, num_workers=32, pin_memory=True)\n",
    "\n",
    "test_input, test_label = next(iter(train_dataloader))\n",
    "input_dim = test_input.shape[2]\n",
    "output_dim = test_label.shape[2]\n",
    "\n",
    "model = AVDCEncoderOnlyModel(input_dim=input_dim,\n",
    "                              output_dim=output_dim,\n",
    "                              d_model=d_model,\n",
    "                              dim_feedforward=dim_feedforward,\n",
    "                              num_heads=num_heads,\n",
    "                              num_encoder_layers=num_encoder_layers,\n",
    "                              dropout=dropout).to(device)\n",
    "\n",
    "print(\"Number of parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=100000)\n",
    "\n",
    "will_train = False\n",
    "will_validate = True\n",
    "will_load_checkpoint = True\n",
    "write_test_data = True\n",
    "visualize_epochs = 25\n",
    "\n",
    "if write_test_data:\n",
    "    visualize_epochs = 1\n",
    "\n",
    "use_wandb = True\n",
    "if not will_train:\n",
    "    use_wandb = False\n",
    "if use_wandb:\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"avdc\",\n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "            \"input_dim\": input_dim,\n",
    "            \"d_model\": d_model,\n",
    "            \"dim_feedforward\": dim_feedforward,\n",
    "            \"num_heads\": num_heads,\n",
    "            \"num_encoder_layers\": num_encoder_layers,\n",
    "            \"num_decoder_layers\": num_decoder_layers,\n",
    "            \"dropout\": dropout,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"learning_rate\": learning_rate\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Load checkpoint\n",
    "starting_epoch = 0\n",
    "if will_load_checkpoint:\n",
    "    # Find the latest checkpoint in ./checkpoints by modified time\n",
    "    list_of_files = glob.glob('./checkpoints/*')\n",
    "    if len(list_of_files) == 0:\n",
    "        print(\"No checkpoints found.\")\n",
    "    else:\n",
    "        latest_file = max(list_of_files, key=os.path.getctime)\n",
    "        print(f\"Loading checkpoint: {latest_file}\")\n",
    "        starting_epoch = int(latest_file.split('_')[-1].split('.')[0])\n",
    "        model.load_state_dict(torch.load(latest_file))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    if epoch < starting_epoch:\n",
    "        continue\n",
    "\n",
    "    avg_train_loss = 0\n",
    "    if will_train:\n",
    "        avg_train_loss = evaluate(epoch, model, train_dataloader, optimizer)\n",
    "   \n",
    "    avg_val_loss = 0\n",
    "    if will_validate:\n",
    "        avg_val_loss = evaluate(epoch, model, val_dataloader, None)\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{epochs}], LR: {scheduler.get_last_lr()}, '\n",
    "          f'Train Loss: {avg_train_loss:.8f}, Val Loss: {avg_val_loss:.8f}')\n",
    "\n",
    "    # Save checkpoint\n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(model.state_dict(), f'checkpoints/checkpoint_{epoch}.pt')\n",
    "\n",
    "    # if epoch % 50 == 0:\n",
    "    #     pdb.set_trace()\n",
    "\n",
    "    if use_wandb:\n",
    "        wandb.log({\"epoch\": epoch,\n",
    "                   \"train_loss\": avg_train_loss, \"val_loss\": avg_val_loss})\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb6bcf0d5c59a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
