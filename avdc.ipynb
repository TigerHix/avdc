{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21f85eb1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T00:49:39.458399Z",
     "start_time": "2024-05-02T00:49:35.696847Z"
    }
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import roma\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f397081de0f9c68",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Generate Dataset\n",
    "Note: you don't (and probably can't) run this; a generated dataset is under `./dataset`.\n",
    "\n",
    "We convert all euler angles to quaternions which are more stable for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e46ab3b7b0381d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T00:47:49.927221Z",
     "start_time": "2024-05-02T00:47:49.919772Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "mmd_path = '/mnt/c/research/avdc/mmd_processed2'\n",
    "sliding_window_size = 10\n",
    "val_ratio = 0.05\n",
    "\n",
    "train_data = []\n",
    "val_data = []\n",
    "\n",
    "will_generate = False\n",
    "\n",
    "def process(folder_path):\n",
    "    # Iterate through all json files in the folder\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith('.json'):\n",
    "            with open(folder_path + '/' + file, 'r') as f:\n",
    "                shots = json.load(f)\n",
    "    \n",
    "                duration = 0\n",
    "                for shot in shots:\n",
    "                    duration += shot['duration']\n",
    "            \n",
    "                array = np.zeros((len(shots), 1 + 3 * 4 + 4 * 4))\n",
    "                for i, shot in enumerate(shots):\n",
    "                    array[i, 0] = shot['duration']\n",
    "                    array[i, 1:4] = shot['character_pos']['x'], shot['character_pos']['y'], shot['character_pos']['z']\n",
    "                    array[i, 4:7] = shot['character_pos_offset']['x'], shot['character_pos_offset']['y'], shot['character_pos_offset']['z']\n",
    "                    array[i, 7] = torch.deg2rad(torch.tensor(shot['character_rot_y']))\n",
    "                    array[i, 8] = torch.deg2rad(torch.tensor(shot['character_rot_y_offset']))\n",
    "                    \n",
    "                    array[i, 9:12] = shot['camera_pos']['x'], shot['camera_pos']['y'], shot['camera_pos']['z']\n",
    "                    array[i, 12:15] = shot['camera_pos_offset']['x'], shot['camera_pos_offset']['y'], shot['camera_pos_offset']['z']\n",
    "                    array[i, 15:18] = torch.deg2rad(torch.tensor(shot['camera_rot']['x'])), torch.deg2rad(torch.tensor(shot['camera_rot']['y'])), torch.deg2rad(torch.tensor(shot['camera_rot']['z']))\n",
    "                    array[i, 18:21] = torch.deg2rad(torch.tensor(shot['camera_rot_offset']['x'])), torch.deg2rad(torch.tensor(shot['camera_rot_offset']['y'])), torch.deg2rad(torch.tensor(shot['camera_rot_offset']['z']))\n",
    "                    array[i, 21] = shot['camera_distance']\n",
    "                    array[i, 22] = shot['camera_distance_offset']\n",
    "                    array[i, 23] = shot['camera_fov']\n",
    "                    array[i, 24] = shot['camera_fov_offset']\n",
    "                    \n",
    "                for i in range(len(array) - sliding_window_size + 1):\n",
    "                    data = train_data if np.random.rand() > val_ratio else val_data\n",
    "                    data.append(array[i:i+sliding_window_size])\n",
    "    \n",
    "    return True\n",
    "\n",
    "if will_generate:\n",
    "    processed = 0\n",
    "    for folder in os.listdir(mmd_path):\n",
    "        processed += 1 if process(mmd_path + '/' + folder) else 0\n",
    "\n",
    "    print(\"Generated \", len(train_data), \"training samples and \", len(val_data), \"validation samples from \", processed, \"folders.\")\n",
    "\n",
    "    np_data = np.array(train_data)\n",
    "    np.save('./dataset/train.npy', np_data)\n",
    "    np_data = np.array(val_data)\n",
    "    np.save('./dataset/val.npy', np_data)\n",
    "else:\n",
    "    print(\"Skipped generation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73445189af15dff5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Dataset Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9df863",
   "metadata": {},
   "source": [
    "The data format is as follows:\n",
    "- Inputs: shot duration, character position, character rotation Y\n",
    "- Outputs: camera look at position, camera look at position delta, camera local rotation, camera local rotation delta, camera distance from look at, camera distance from look at delta\n",
    "\n",
    "The output parameters can then be used to generate the camera trajectory. Please refer to the paper for more details.\n",
    "\n",
    "Note camera FoV is included but not used in this paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affc13680a5b5c95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T00:48:01.509458Z",
     "start_time": "2024-05-02T00:47:49.929040Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class AVDCDataset(Dataset):\n",
    "    def __init__(self, dataset_path):\n",
    "        self.shots = []\n",
    "        self.load_dataset(dataset_path)\n",
    "\n",
    "    def load_dataset(self, dataset_path):\n",
    "        self.shots = np.load(dataset_path)\n",
    "        self.shots = torch.from_numpy(self.shots).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.shots.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        shots = self.shots[idx]\n",
    "        inputs = shots[:, 0:9]\n",
    "        labels = shots[:, 9:25]\n",
    "        return inputs, labels\n",
    "\n",
    "# Model, loss, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Data loading\n",
    "train_dataset = AVDCDataset(\"./dataset/train.npy\")\n",
    "val_dataset = AVDCDataset(\"./dataset/val.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948161eba15fa87c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T00:48:01.514723Z",
     "start_time": "2024-05-02T00:48:01.511303Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print data for inspection\n",
    "train_dataset[0]\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff54a41",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f01d90a",
   "metadata": {},
   "source": [
    "Following we implement mathematic utilities, especially the `batch_get_camera_position_and_rotation` function, for computing the final camera transform based on output parameters. For more details, please refer to the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814f56b2e7d04616",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T00:48:01.525443Z",
     "start_time": "2024-05-02T00:48:01.515934Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def angle_normalize(angle):\n",
    "    return (angle + math.pi) % (2 * math.pi) - math.pi\n",
    "\n",
    "def batch_euler_xyz_to_zxy(euler):\n",
    "    return torch.stack([euler[..., 2], euler[..., 0], euler[..., 1]], dim=-1)\n",
    "\n",
    "def batch_euler_zxy_to_xyz(euler):\n",
    "    return torch.stack([euler[..., 1], euler[..., 2], euler[..., 0]], dim=-1)\n",
    "\n",
    "def batch_euler_normalize(euler):\n",
    "    return torch.stack([angle_normalize(euler[..., 0]), angle_normalize(euler[..., 1]), angle_normalize(euler[..., 2])], dim=-1)\n",
    "\n",
    "def batch_get_camera_position_and_rotation(character_pos, character_rot_y, camera_pos, camera_rot, distance):\n",
    "    convention = 'zxy'\n",
    "    zeros = torch.zeros_like(character_rot_y)\n",
    "    pis = torch.ones_like(character_rot_y) * math.pi\n",
    "    character_rot = torch.stack([zeros, character_rot_y, zeros], dim=-1)\n",
    "    camera_rot[..., 1] = -camera_rot[..., 1]\n",
    "    \n",
    "    character_rotation = roma.euler_to_unitquat(convention, batch_euler_xyz_to_zxy(character_rot))\n",
    "    local_camera_rotation = roma.euler_to_unitquat(convention, batch_euler_xyz_to_zxy(camera_rot))\n",
    "    extra_rotation = roma.euler_to_unitquat(convention, batch_euler_xyz_to_zxy(torch.stack([zeros, pis, zeros], dim=-1)))\n",
    "    \n",
    "    combined_rotation = roma.quat_product(roma.quat_product(character_rotation, local_camera_rotation), extra_rotation)\n",
    "    \n",
    "    world_camera_pos = character_pos + roma.quat_action(character_rotation, camera_pos)\n",
    "    camera_backward = roma.quat_action(combined_rotation, torch.stack([zeros, zeros, -distance], dim=-1))\n",
    "    final_camera_position = world_camera_pos + camera_backward\n",
    "    \n",
    "    final_camera_rotation = batch_euler_zxy_to_xyz(roma.unitquat_to_euler(convention, combined_rotation))\n",
    "    \n",
    "    return final_camera_position, final_camera_rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec8afce",
   "metadata": {},
   "source": [
    "Following we implement the `calculate_angle` function which computes the angle difference between the vector `character - cam` and `forward_vector(cam) - cam`. Essentially, how much the camera has to rotate to center the character in the view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a16793cd513921",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T00:48:01.572077Z",
     "start_time": "2024-05-02T00:48:01.527048Z"
    }
   },
   "outputs": [],
   "source": [
    "character_chest_height = 1.115\n",
    "\n",
    "def angle_v2(a, b, dim=-1, eps=1e-5):\n",
    "    a_norm = a.norm(dim=dim, keepdim=True)\n",
    "    b_norm = b.norm(dim=dim, keepdim=True)\n",
    "\n",
    "    a_prime = (a * b_norm - a_norm * b).norm(dim=dim)\n",
    "    b_prime = (a * b_norm + a_norm * b).norm(dim=dim)\n",
    "    \n",
    "    mask = (a_prime < eps) & (b_prime < eps)\n",
    "    a_adjusted = torch.where(mask, a_prime + eps, a_prime)\n",
    "    b_adjusted = torch.where(mask, b_prime + eps, b_prime)\n",
    "\n",
    "    return 2 * torch.atan2(a_adjusted, b_adjusted)\n",
    "\n",
    "def calculate_angle(camera_pos, camera_rot, character_pos):\n",
    "    chest_pos = character_pos.clone()\n",
    "    chest_pos[..., 1] = character_chest_height\n",
    "    \n",
    "    zeros = torch.zeros(character_pos.shape[:-1], device=character_pos.device)\n",
    "    ones = torch.ones(character_pos.shape[:-1], device=character_pos.device)\n",
    "    \n",
    "    dir_vec = chest_pos - camera_pos\n",
    "    dir_norm = torch.norm(dir_vec, dim=-1, keepdim=True)\n",
    "    dir_vec = dir_vec / (dir_norm + 1e-5)  # Add epsilon to avoid division by zero\n",
    "    camera_dir = roma.quat_action(roma.euler_to_unitquat('zxy', batch_euler_xyz_to_zxy(camera_rot)), torch.stack([zeros, zeros, ones], dim=-1))\n",
    "    angle = angle_v2(camera_dir, dir_vec, dim=-1)\n",
    "    return angle\n",
    "      \n",
    "\n",
    "# Define the test inputs\n",
    "camera_pos_test = torch.tensor([[-3.11, 1.15, 2.01], [-1.77, 1.34, 3.77]], dtype=torch.float32)\n",
    "camera_rot_test = torch.deg2rad(torch.tensor([[2.30, 112.65, 0.00], [354.39, 37.95, 0.00]], dtype=torch.float32))  # No rotation and 90 degrees yaw\n",
    "character_pos_test = torch.tensor([[0.00, 1.11, 0.06], [0.00, 1.11, 0.06]], dtype=torch.float32)\n",
    "size = 2\n",
    "camera_pos_test = camera_pos_test.unsqueeze(0).expand(size, -1, -1).contiguous()\n",
    "camera_rot_test = camera_rot_test.unsqueeze(0).expand(size, -1, -1).contiguous()\n",
    "character_pos_test = character_pos_test.unsqueeze(0).expand(size, -1, -1).contiguous()\n",
    "\n",
    "# Run the test\n",
    "calculate_angle(camera_pos_test, camera_rot_test, character_pos_test)\n",
    "\n",
    "# Expected: 0.1675, 2.0369"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c406eea7",
   "metadata": {},
   "source": [
    "Following we implement the `calculate_look_angle` function which computes the Y rotation difference between the camera orientation and the character orientation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f1891a954b85d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T00:48:01.579928Z",
     "start_time": "2024-05-02T00:48:01.573148Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_look_angle(camera_rot, character_rot_y):\n",
    "    zeros = torch.zeros_like(character_rot_y)\n",
    "    ones = torch.ones_like(character_rot_y)\n",
    "    \n",
    "    character_rot = torch.stack([zeros, character_rot_y, zeros], dim=-1)\n",
    "    \n",
    "    camera_dir = roma.quat_action(roma.euler_to_unitquat('zxy', batch_euler_xyz_to_zxy(camera_rot)), torch.stack([zeros, zeros, ones], dim=-1))\n",
    "    character_dir = roma.quat_action(roma.euler_to_unitquat('zxy', batch_euler_xyz_to_zxy(character_rot)), torch.stack([zeros, zeros, ones], dim=-1))\n",
    "    angle = angle_v2(camera_dir, character_dir, dim=-1)\n",
    "    return angle\n",
    "\n",
    "torch.rad2deg(calculate_look_angle(torch.tensor([[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], dtype=torch.float32), torch.deg2rad(torch.tensor([190.0, 170.0], dtype=torch.float32))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380ac79de67cd4c1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d882b58f32a460e4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-02T00:48:01.593423Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.pe[:, : x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class AVDCEncoderOnlyModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model, dim_feedforward, num_heads, num_encoder_layers, dropout=0.1):\n",
    "        super(AVDCEncoderOnlyModel, self).__init__()\n",
    "\n",
    "        self.input_embedding = nn.Linear(input_dim, d_model)\n",
    "        self.input_positional_encoding = PositionalEncoding(d_model)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_encoder_layers)\n",
    "        self.out = nn.Linear(d_model, output_dim)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.input_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.out.weight)\n",
    "        self.input_embedding.bias.data.fill_(0.01)\n",
    "        self.out.bias.data.fill_(0.01)\n",
    "        \n",
    "        # Init transformer weights\n",
    "        for p in self.transformer_encoder.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_embedding(x)\n",
    "        x = self.input_positional_encoding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e620d02",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "We design a heuristic loss function that penalizes on stylistic features of the camera trajectory. For more details, please refer to the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc37132",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_loss(inputs, outputs, labels, step, total_step, inference=False):\n",
    "    pos_weight = 2\n",
    "    y_weight = 4\n",
    "    rot_weight = 1\n",
    "    distance_weight = 2\n",
    "    d_weight = 2\n",
    "    angle_weight = 1\n",
    "    y_angle_weight = 1\n",
    "    bb_weight = 2\n",
    "    \n",
    "    input_character_pos = inputs[:, :, 1:4]\n",
    "    input_character_pos_offset = inputs[:, :, 4:7]\n",
    "    input_character_rot_y = inputs[:, :, 7]\n",
    "    input_character_rot_y_offset = inputs[:, :, 8]\n",
    "    \n",
    "    input_character_pos0 = input_character_pos\n",
    "    input_character_pos1 = input_character_pos + input_character_pos_offset\n",
    "    \n",
    "    input_character_rot_y0 = input_character_rot_y\n",
    "    input_character_rot_y1 = input_character_rot_y + input_character_rot_y_offset\n",
    "    \n",
    "    outputs_camera_pos = outputs[:, :, 0:3]\n",
    "    outputs_camera_pos_offset = outputs[:, :, 3:6]\n",
    "    outputs_camera_pos_0 = outputs_camera_pos\n",
    "    outputs_camera_pos_1 = outputs_camera_pos + outputs_camera_pos_offset\n",
    "    \n",
    "    outputs_camera_rot = outputs[:, :, 6:9]\n",
    "    outputs_camera_rot_offset = outputs[:, :, 9:12]\n",
    "    outputs_camera_rot_0 = outputs_camera_rot\n",
    "    outputs_camera_rot_1 = outputs_camera_rot + outputs_camera_rot_offset\n",
    "    \n",
    "    outputs_camera_d = outputs[:, :, 12]\n",
    "    outputs_camera_d_offset = outputs[:, :, 13]\n",
    "    outputs_camera_d_0 = outputs_camera_d\n",
    "    outputs_camera_d_1 = outputs_camera_d + outputs_camera_d_offset\n",
    "    \n",
    "    labels_camera_pos = labels[:, :, 0:3]\n",
    "    labels_camera_pos_offset = labels[:, :, 3:6]\n",
    "    labels_camera_pos0 = labels_camera_pos\n",
    "    labels_camera_pos1 = labels_camera_pos + labels_camera_pos_offset\n",
    "    \n",
    "    labels_camera_rot = labels[:, :, 6:9]\n",
    "    labels_camera_rot_offset = labels[:, :, 9:12]\n",
    "    labels_camera_rot0 = labels_camera_rot\n",
    "    labels_camera_rot1 = labels_camera_rot + labels_camera_rot_offset\n",
    "    \n",
    "    labels_camera_d = labels[:, :, 12]\n",
    "    labels_camera_d_offset = labels[:, :, 13]\n",
    "    labels_camera_d0 = labels_camera_d\n",
    "    labels_camera_d1 = labels_camera_d + labels_camera_d_offset\n",
    "    \n",
    "    outputs_start_camera_pos = outputs_camera_pos_0\n",
    "    outputs_start_camera_rot = outputs_camera_rot_0\n",
    "    \n",
    "    labels_start_camera_pos = labels_camera_pos0\n",
    "    labels_start_camera_rot = labels_camera_rot0\n",
    "    \n",
    "    outputs_end_camera_pos = outputs_camera_pos_1\n",
    "    outputs_end_camera_rot = outputs_camera_rot_1\n",
    "    \n",
    "    labels_end_camera_pos = labels_camera_pos1\n",
    "    labels_end_camera_rot = labels_camera_rot1\n",
    "    \n",
    "    outputs_start_camera_pos, outputs_start_camera_rot = batch_get_camera_position_and_rotation(input_character_pos0, input_character_rot_y0, outputs_camera_pos_0, outputs_camera_rot_0, outputs_camera_d_0)\n",
    "    \n",
    "    labels_start_camera_pos, labels_start_camera_rot = batch_get_camera_position_and_rotation(input_character_pos0, input_character_rot_y0, labels_camera_pos0, labels_camera_rot0, labels_camera_d0)\n",
    "    \n",
    "    outputs_end_camera_pos, outputs_end_camera_rot = batch_get_camera_position_and_rotation(input_character_pos1, input_character_rot_y1, outputs_camera_pos_1, outputs_camera_rot_1, outputs_camera_d_1)\n",
    "    \n",
    "    labels_end_camera_pos, labels_end_camera_rot = batch_get_camera_position_and_rotation(input_character_pos1, input_character_rot_y1, labels_camera_pos1, labels_camera_rot1, labels_camera_d1)\n",
    "    \n",
    "    # ensure actual position delta is similar to true position delta\n",
    "    actual_pos_delta_loss = torch.nn.functional.mse_loss((outputs_end_camera_pos - outputs_start_camera_pos).norm(dim=2).mean(dim=1), (labels_end_camera_pos - labels_start_camera_pos).norm(dim=2).mean(dim=1), reduction='mean')\n",
    "\n",
    "    # ensures predicted position delta is similar to true position delta (L2 loss)\n",
    "    pos_delta_loss = torch.nn.functional.mse_loss(outputs_camera_pos_offset.norm(dim=2).mean(dim=1), labels_camera_pos_offset.norm(dim=2).mean(dim=1), reduction='mean')\n",
    "    \n",
    "    # ensures predicted rotation delta is similar to true rotation delta (L2 loss)\n",
    "    rot_delta_loss = torch.nn.functional.mse_loss(outputs_camera_rot_offset.norm(dim=2).mean(dim=1), labels_camera_rot_offset.norm(dim=2).mean(dim=1), reduction='mean')\n",
    "    \n",
    "    # ensures predicted start camera position has similar distance to character\n",
    "    predicted_start_dist_to_char = outputs_start_camera_pos - input_character_pos0\n",
    "    true_start_dist_to_char = labels_start_camera_pos - input_character_pos0\n",
    "    start_distance_loss = torch.nn.functional.mse_loss(predicted_start_dist_to_char, true_start_dist_to_char, reduction='mean')\n",
    "\n",
    "    predicted_end_dist_to_char = outputs_end_camera_pos - input_character_pos1\n",
    "    true_end_dist_to_char = labels_end_camera_pos - input_character_pos1\n",
    "    end_distance_loss = torch.nn.functional.mse_loss(predicted_end_dist_to_char, true_end_dist_to_char, reduction='mean')\n",
    "\n",
    "    # ensures predicted d is similar to true d\n",
    "    d_loss = torch.nn.functional.mse_loss(outputs_camera_d.mean(), labels_camera_d.mean(), reduction='mean') + torch.nn.functional.mse_loss(outputs_camera_d_offset.mean(), labels_camera_d_offset.mean(), reduction='mean')\n",
    "    \n",
    "    # ensures predicted start camera rotation has similar angle\n",
    "    predicted_start_angle = calculate_angle(outputs_start_camera_pos, outputs_start_camera_rot, input_character_pos)\n",
    "    true_start_angle = calculate_angle(labels_start_camera_pos, labels_start_camera_rot, input_character_pos)\n",
    "    start_angle_loss = torch.nn.functional.mse_loss(predicted_start_angle, true_start_angle, reduction='mean')\n",
    "    \n",
    "    # ensures predicted end camera rotation has similar angle\n",
    "    predicted_end_angle = calculate_angle(outputs_end_camera_pos, outputs_end_camera_rot, input_character_pos1)\n",
    "    true_end_angle = calculate_angle(labels_end_camera_pos, labels_end_camera_rot, input_character_pos1)\n",
    "    end_angle_loss = torch.nn.functional.mse_loss(predicted_end_angle, true_end_angle, reduction='mean')\n",
    "    \n",
    "    # ensures camera start y is overall similar\n",
    "    predicted_start_y = outputs_start_camera_pos[:, :, 1].mean(dim=1)\n",
    "    true_start_y = labels_start_camera_pos[:, :, 1].mean(dim=1)\n",
    "    start_y_loss = torch.nn.functional.mse_loss(predicted_start_y, true_start_y, reduction='mean')\n",
    "    \n",
    "    # ensures camera end y is similar\n",
    "    predicted_end_y = outputs_end_camera_pos[:, :, 1].mean(dim=1)\n",
    "    true_end_y = labels_end_camera_pos[:, :, 1].mean(dim=1)\n",
    "    end_y_loss = torch.nn.functional.mse_loss(predicted_end_y, true_end_y, reduction='mean')\n",
    "    \n",
    "    # ensures camera Y rotation and player Y rotation start difference is similar\n",
    "    predicted_start_y_rot_diff = calculate_look_angle(outputs_start_camera_rot, input_character_rot_y0)\n",
    "    true_start_y_rot_diff = calculate_look_angle(labels_start_camera_rot, input_character_rot_y0)\n",
    "    start_y_rot_diff_loss = (predicted_start_y_rot_diff - true_start_y_rot_diff) ** 2\n",
    "\n",
    "    # ensures camera Y rotation and player Y rotation end difference is similar\n",
    "    predicted_end_y_rot_diff = calculate_look_angle(outputs_end_camera_rot, input_character_rot_y1)\n",
    "    true_end_y_rot_diff = calculate_look_angle(labels_end_camera_rot, input_character_rot_y1)\n",
    "    end_y_rot_diff_loss = (predicted_end_y_rot_diff - true_end_y_rot_diff) ** 2\n",
    "    \n",
    "    def bounding_box_size(tensor):\n",
    "        min = tensor.min(dim=1).values\n",
    "        max = tensor.max(dim=1).values\n",
    "        return max - min    \n",
    "    \n",
    "    # ensures predicted start position has similar bounding box size\n",
    "    predicted_start_bounding_box_size = bounding_box_size(outputs_start_camera_pos)\n",
    "    true_start_bounding_box_size = bounding_box_size(labels_start_camera_pos)\n",
    "    start_bounding_box_size_loss = torch.nn.functional.mse_loss(predicted_start_bounding_box_size.norm(dim=1), true_start_bounding_box_size.norm(dim=1), reduction='mean')\n",
    "    \n",
    "    # ensures predicted end position has similar bounding box size\n",
    "    predicted_end_bounding_box_size = bounding_box_size(outputs_end_camera_pos)\n",
    "    true_end_bounding_box_size = bounding_box_size(labels_end_camera_pos)\n",
    "    end_bounding_box_size_loss = torch.nn.functional.mse_loss(predicted_end_bounding_box_size.norm(dim=1), true_end_bounding_box_size.norm(dim=1), reduction='mean')\n",
    "\n",
    "    pos_loss = pos_weight * pos_delta_loss.mean()\n",
    "    rot_loss = rot_weight * rot_delta_loss.mean()\n",
    "    d_loss = d_weight * d_loss.mean()\n",
    "    distance_loss = distance_weight * (start_distance_loss + end_distance_loss)\n",
    "    angle_loss = angle_weight * (start_angle_loss.mean() + end_angle_loss.mean())\n",
    "    y_angle_loss = y_angle_weight * (start_y_rot_diff_loss.mean() + end_y_rot_diff_loss.mean())\n",
    "    bb_loss = bb_weight * (start_bounding_box_size_loss + end_bounding_box_size_loss)\n",
    "    y_loss = y_weight * (start_y_loss.mean() + end_y_loss.mean())\n",
    "\n",
    "    loss = pos_loss + rot_loss + d_loss + distance_loss + angle_loss + y_angle_loss + bb_loss + y_loss\n",
    "\n",
    "    if use_wandb and not inference:\n",
    "        wandb.log({\"step\": total_step, \"loss\": loss, \"pos_loss\": pos_loss, \"rot_loss\": rot_loss, \"d_loss\": d_loss, \"distance_loss\": distance_loss, \"angle_loss\": angle_loss, \"y_angle_loss\": y_angle_loss, \"bb_loss\": bb_loss,\n",
    "        \"y_loss\": y_loss})\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdc14f2",
   "metadata": {},
   "source": [
    "## Training Code & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeb6e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(inputs, outputs, labels):\n",
    "    rand_idx = np.random.randint(0, outputs.shape[0])\n",
    "    inputs = inputs[rand_idx]\n",
    "    outputs = outputs[rand_idx]\n",
    "    labels = labels[rand_idx]\n",
    "    \n",
    "    # Draw camera path on XZ plane\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.ioff()\n",
    "    \n",
    "    character_pos_x_from = inputs[:, 1].cpu().detach().numpy()\n",
    "    character_pos_z_from = inputs[:, 3].cpu().detach().numpy()\n",
    "    character_pos_x_to = character_pos_x_from + inputs[:, 4].cpu().detach().numpy()\n",
    "    character_pos_z_to = character_pos_z_from + inputs[:, 6].cpu().detach().numpy()\n",
    "    \n",
    "    character_pos = inputs[:, 1:4]\n",
    "    character_pos_offset = inputs[:, 4:7]\n",
    "    character_rot_y = inputs[:, 7]\n",
    "    character_rot_y_offset = inputs[:, 8]\n",
    "    \n",
    "    output_camera_pos = outputs[:, 0:3]\n",
    "    output_camera_pos_offset = outputs[:, 3:6]\n",
    "    output_camera_rot = outputs[:, 6:9]\n",
    "    output_camera_rot_offset = outputs[:, 9:12]\n",
    "    output_camera_d = outputs[:, 12]\n",
    "    output_camera_d_offset = outputs[:, 13]\n",
    "    \n",
    "    labels_camera_pos = labels[:, 0:3]\n",
    "    labels_camera_pos_offset = labels[:, 3:6]\n",
    "    labels_camera_rot = labels[:, 6:9]\n",
    "    labels_camera_rot_offset = labels[:, 9:12]\n",
    "    labels_camera_d = labels[:, 12]\n",
    "    labels_camera_d_offset = labels[:, 13]\n",
    "    \n",
    "    num_paths = outputs.shape[0]\n",
    "    for i in range(num_paths):\n",
    "        ax.plot([character_pos_x_from[i], character_pos_x_to[i]], [character_pos_z_from[i], character_pos_z_to[i]], label=\"Character path\", color=np.array([0, 1, 0, i / num_paths]))\n",
    "        \n",
    "        # sample camera curve path\n",
    "        def sample(character_pos, character_pos_offset, character_rot_y, character_rot_y_offset, pos, pos_offset, rot, rot_offset, d, d_offset, color):\n",
    "            def sample_camera_transform(character_pos, character_pos_offset, character_rot_y, character_rot_y_offset, pos, pos_offset, rot, rot_offset, d, d_offset, t):\n",
    "                cp = character_pos + t * character_pos_offset\n",
    "                cry = (character_rot_y + t * character_rot_y_offset).squeeze(-1)\n",
    "                p = pos + t * pos_offset\n",
    "                r = rot + t * rot_offset\n",
    "                d = (d + t * d_offset).squeeze(-1)\n",
    "                out_pos, out_rot = batch_get_camera_position_and_rotation(cp, cry, p, r, d)\n",
    "                return out_pos, out_rot\n",
    "        \n",
    "            t = torch.linspace(0, 1, steps=100).unsqueeze(1).to(device)\n",
    "            out_pos, out_rot = sample_camera_transform(character_pos, character_pos_offset, character_rot_y, character_rot_y_offset, pos, pos_offset, rot, rot_offset, d, d_offset, t)\n",
    "            \n",
    "            out_pos = out_pos.cpu().detach().numpy()\n",
    "            ax.plot(out_pos[:, 0], out_pos[:, 2], color=color)\n",
    "        \n",
    "        sample(character_pos[i], character_pos_offset[i], character_rot_y[i], character_rot_y_offset[i],\n",
    "            output_camera_pos[i], output_camera_pos_offset[i], output_camera_rot[i], output_camera_rot_offset[i], output_camera_d[i], output_camera_d_offset[i], np.array([1, 0, 0, i / num_paths]))\n",
    "        sample(character_pos[i], character_pos_offset[i], character_rot_y[i], character_rot_y_offset[i],\n",
    "            labels_camera_pos[i], labels_camera_pos_offset[i], labels_camera_rot[i], labels_camera_rot_offset[i], labels_camera_d[i], labels_camera_d_offset[i], np.array([0, 0, 1, i / num_paths]))\n",
    "    \n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Z')\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    if write_test_data:\n",
    "        import time\n",
    "        # timestamp as filename\n",
    "        filename = f\"test_data/test_{(time.time() * 1000):.0f}\"\n",
    "        fig.savefig(filename + \".png\")\n",
    "        \n",
    "        json_obj = {\n",
    "            \"inputs\": inputs.cpu().detach().numpy().tolist(),\n",
    "            \"outputs\": outputs.cpu().detach().numpy().tolist(),\n",
    "            \"labels\": labels.cpu().detach().numpy().tolist()\n",
    "        }\n",
    "        with open(f\"{filename}.json\", \"w\") as f:\n",
    "            json.dump(json_obj, f)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def evaluate(epoch, model, dataloader, optimizer):\n",
    "    avg_loss = 0\n",
    "\n",
    "    i = 0\n",
    "    length = len(dataloader)\n",
    "    for inputs, labels in dataloader:\n",
    "        step = epoch * length + i\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = compute_loss(inputs, outputs, labels, i, step, optimizer is None)\n",
    "        l = loss.item()\n",
    "        avg_loss += l\n",
    "        \n",
    "        if (optimizer is None and epoch % visualize_epochs == 0) or (optimizer is not None and (i % 100 == 0 and i > 0) or (epoch % 10 == 0 and i == 0)) or write_test_data:\n",
    "            fig = visualize(inputs, outputs, labels)\n",
    "            if use_wandb:\n",
    "                wandb.log({\"epoch\": epoch, \"path\": wandb.Image(fig)})\n",
    "            plt.close(fig)\n",
    "        \n",
    "        if optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        i += 1\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Step {i}/{length}, Loss: {l}\")\n",
    "\n",
    "    avg_loss /= len(dataloader)\n",
    "\n",
    "    return avg_loss\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.set_printoptions(sci_mode=False, threshold=10000)\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Hyperparameters\n",
    "d_model = 512\n",
    "dim_feedforward = 1024\n",
    "num_heads = 2\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 1\n",
    "dropout = 0.2\n",
    "batch_size = 1024\n",
    "epochs = 100000\n",
    "learning_rate = 5e-5\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=32, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1024, shuffle=False, num_workers=32, pin_memory=True)\n",
    "\n",
    "test_input, test_label = next(iter(train_dataloader))\n",
    "input_dim = test_input.shape[2]\n",
    "output_dim = test_label.shape[2]\n",
    "\n",
    "model = AVDCEncoderOnlyModel(input_dim=input_dim,\n",
    "                              output_dim=output_dim,\n",
    "                              d_model=d_model,\n",
    "                              dim_feedforward=dim_feedforward,\n",
    "                              num_heads=num_heads,\n",
    "                              num_encoder_layers=num_encoder_layers,\n",
    "                              dropout=dropout).to(device)\n",
    "\n",
    "print(\"Number of parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=100000)\n",
    "\n",
    "will_train = True\n",
    "will_validate = True\n",
    "will_load_checkpoint = False\n",
    "write_test_data = False\n",
    "visualize_epochs = 20\n",
    "validate_epochs = 10\n",
    "\n",
    "if write_test_data or not will_train:\n",
    "    visualize_epochs = 1\n",
    "\n",
    "use_wandb = True\n",
    "if not will_train:\n",
    "    use_wandb = False\n",
    "if use_wandb:\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"avdc\",\n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "            \"input_dim\": input_dim,\n",
    "            \"d_model\": d_model,\n",
    "            \"dim_feedforward\": dim_feedforward,\n",
    "            \"num_heads\": num_heads,\n",
    "            \"num_encoder_layers\": num_encoder_layers,\n",
    "            \"num_decoder_layers\": num_decoder_layers,\n",
    "            \"dropout\": dropout,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"learning_rate\": learning_rate\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Load checkpoint\n",
    "starting_epoch = 0\n",
    "if will_load_checkpoint:\n",
    "    # Find the latest checkpoint in ./checkpoints by modified time\n",
    "    list_of_files = glob.glob('./checkpoints/*')\n",
    "    if len(list_of_files) == 0:\n",
    "        print(\"No checkpoints found.\")\n",
    "    else:\n",
    "        latest_file = max(list_of_files, key=os.path.getctime)\n",
    "        print(f\"Loading checkpoint: {latest_file}\")\n",
    "        starting_epoch = int(latest_file.split('_')[-1].split('.')[0])\n",
    "        model.load_state_dict(torch.load(latest_file))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    if epoch < starting_epoch:\n",
    "        continue\n",
    "\n",
    "    avg_train_loss = 0\n",
    "    if will_train:\n",
    "        avg_train_loss = evaluate(epoch, model, train_dataloader, optimizer)\n",
    "   \n",
    "    avg_val_loss = 0\n",
    "    if will_validate and epoch % validate_epochs == 0 and epoch > 0: \n",
    "        avg_val_loss = evaluate(epoch, model, val_dataloader, None)\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{epochs}], LR: {scheduler.get_last_lr()}, '\n",
    "          f'Train Loss: {avg_train_loss:.8f}, Val Loss: {avg_val_loss:.8f}')\n",
    "\n",
    "    # Save checkpoint\n",
    "    if epoch % 1 == 0:\n",
    "        torch.save(model.state_dict(), f'checkpoints/checkpoint_{epoch}.pt')\n",
    "\n",
    "    if use_wandb:\n",
    "        wandb.log({\"epoch\": epoch,\n",
    "                   \"train_loss\": avg_train_loss, \"val_loss\": avg_val_loss}\n",
    "                   if will_validate else {\"epoch\": epoch, \"train_loss\": avg_train_loss})\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
